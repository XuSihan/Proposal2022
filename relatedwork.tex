\subsection{国内外相关工作}\label{relatedwork}

% 根据与本项目的相关性，本节从覆盖充分性指标、神经网络测试数据生成、深度学习框架测试以及第三方组件漏洞挖掘四个方面介绍和分析国内外研究现状。

根据与本项目的相关性，本节从可解释性研究、测试度量指标、测试集生成和三个方面介绍
和分析国内外相关工作。

\subsubsection{可解释性研究}
{人工智能的可解释性研究是最近几年人工智能领域和软件工程领域（可信人工智能）的研
	究热点}。浙江大学的纪守领老师团队对于目前机器学习和深度学习模型的可解释性研究进
行了详细的调研和总结，调研表明\textbf{目前人工智能的可解释性研究仍旧处于初级阶
	段，并且学术界对深度学习模型的可解释性仍旧缺乏统一的认识}\citess{jishouling}。模
型可解释性总体上可分为\textbf{事前可解释性和事后可解释性}。其中，事前可
解释性指通过训练结构简单、可解释性强的模型或将可解释性结合到模型结构中，使模型本
身具备可解释能力。事后可解释是指通过可解释性方法来对已经训练好的模型进行解释
\citess{jishouling}。

事前可解释性无需额外的信息就可以理解模型的决策过程或决策依据，通常采用结构简单、
易于理解的模型来实现，例如线性回归、决策树、朴素贝叶斯等。事前可解释性要求模型的
决策过程或决策依据可模拟，模型的每一个输入以及每一维特征都有直观的解释
~\citess{lou2012intelligible}。对于朴素贝叶斯模型的预测，可以很容易地转化为单个
特征值的贡献~\citess{strumbelj2010efficient}；对于线性模型，其权重则直接反映了特
征重要性~\citess{ribeiro2016should}；对于决策树模型，每一条决策路径所对应的条件
规则都为最终的分类结果提供了解释~\citess{huysmans2011empirical}。为了提高简单线
性模型的准确率，同时保留其事前可解释性，广义加性模型通过简单的线性函数组合每个单
特征模型得到最终决策~\citess{wood2006generalized}。另外，事前可解释性也可通过引
入注意力机制来达到，注意力权重矩阵直接体现了模型在决策过程中感兴趣的区域，从而对
模型进行一定程度的解释~\citess{zhang2021context}。

事后可解释性发生在模型训练之后，通过构建解释方法对模型的工作机制、决策行为和决策
依据进行解释。早期针对事后可解释性的研究主要通过规则提取得到对复杂模型整体决策逻
辑的理解~\citess{bastani2017interpreting}。然而，规则提取只能提供近似解释，不一
定能反映待解释模型的真实行为，且受规则复杂度的制约。当模型的结构过于复杂时，要想
从整体上理解模型的决策逻辑通常是很困难的，解决该问题的一个有效途径是降低待解释模
型的复杂度，而\textbf{知识蒸馏(knowledge distillation)则是降低模型复杂度，提高模型可解释
	性的有效方法~\citess{gou2021knowledge}}。{由于知识蒸馏可以完成从教师模型到学生模
型的知识迁移，因而学生模型可以看作是教师模型的全局近似，在一定程度上反映了教师模
型的整体逻辑，因此可以基于学生模型，提供对教师模型的全局解释}。此外，由于整体解
释模型行为较为困难，部分研究以输入样本为导向，通过分析输入样本的每一维特征对模型
最终决策结果的贡献来实现事后可解释性，典型的方法包括敏感性分析解释
~\citess{robnik2008explaining}、局部近似解释~\citess{ribeiro2018anchors}、梯度反
向传播解释~\citess{ding2017visualizing}、特征反演解释
~\citess{dosovitskiy2016inverting}等。然而，现有解释方法很难保证解释结果的准确性
和一致性，为此，Chu等人~\citess{chu2018exact}提出了一种解释方法，可为分段线性神
经网络家族模型提供精确一致的解释，但该方法只能解释线性神经网络模型，无法应用于解
释非线性神经网络模型。

\textbf{尽管已经有很多针对机器学习和深度学习模型本身的可解释性研究，但是对于本项目关注的针对深度学习模型测试的可解释性研
	究工作还十分有限。}Kim等人\citess{Kim2019Guiding}提出一种基于“意外度”的测试覆盖指标，通过测试数据与训练集在隐藏
层表示上的距离，估算测试数据导致模型预测错误的可能性，以此对测试充分性进行可解释分析。Xie等人\citess{Xie2021NPC}
提出了神经元路径覆盖指标，基于神经网络中决策图的控制流和数据流，提出了两种路径覆盖的变体来衡量测试集的充分性。虽
然这些方法在一定程度上提供了针对深度学习模型的测试覆盖指标的解释，但他们只局限于模型推理时神经元的取值，对模型决
策行为和决策依据的解释不足，测试人员难以根据测试结果判断模型是否在以符合人类认知的形式正常工作，也无法确认测试集
对模型决策行为的覆盖性。\textbf{因此，可解释性方法的能力在深度学习模型测试领域并没有得到准确的说明和研究}。

\iffalse
	\textbf{尽管已经有很多针对机器学习和深度学习模型本身的可解释性研究，但是对于本项
		目关注的针对深度学习系统测试方面的可解释性研究工作还十分有限。}Kim等人
	\citess{Kim2019Guiding}提出一种基于“意外度”的测试覆盖指标，通过度量测试数据与训
	练集的不同距离，评估测试集对样本输入空间的覆盖度，从而对测试目标进行一定的可解释
	性分析。Xie等人\citess{Xie2021NPC}提出了神经元路径覆盖指标，类似于传统的程序控制
	流图，首先从深度神经网络中提取决策图用来表示模型的决策逻辑，然后基于决策图的控制
	流和数据流，该方法提出了两种路径覆盖的变体来衡量测试数据在执行决策逻辑时的充分
	性。该测试方法在一定程度上反映出模型的决策逻辑，但由于模型本身缺乏可解释性，难以
	从控制流或数据流路径上辅助开发人员找到模型失效的原因，从而帮助修复模型。但是该方
	法依赖于模型推理过程中的神经元取值，缺乏对模型决策行为的解释，难以检查模型是否在
	以符合人类认知的形式正常工作。
\fi

\subsubsection{测试度量指标}
{测试覆盖指标是衡量测试集充分性的标尺，是软件测试的重要研究问题之一。}对于传统软件，若测试集遍历了待测软件所有的语句、分
支和路径，则在一定程度上表明了测试集对待测软件进行了充分性测试\citess{hilton2018large}。对于深度神经网络而言，其本身的高
维连续特性导致测试集很难遍历所有可能的输入空间，为提高测试集多样性，目前有很多研究提出了关于深度神经网络的结构覆盖指标来
指导测试数据生成，主要分为基于\textbf{单个神经元取值和多个神经元取值组合}两类。

%，从不同角度测试衡量测试集对模型的覆盖充分性。


\iffalse
	\cref{tab:coverage_criteria}总结了现有神经网络测试覆盖指标，其中$m$表示训练
	集规模，$n$表示神经元个数，$l$表示神经网络层数。根据覆盖思想的不同，可分为以下六
	类：

	\begin{table}[htp]
		\renewcommand\arraystretch{1.5}
		\small
		\centering
		\caption{现有神经网络覆盖充分性指标}
		\label{tab:coverage_criteria}
		% \begin{tabular}{p{3cm}p{5cm}p{1cm}p{1cm}p{2cm}}
		\begin{tabular}{ccccc}
			\toprule
			\textbf{序号} & \textbf{主要思想}    & \textbf{覆盖指标}               & \textbf{复杂度} & \textbf{文献号}                                \\
			\midrule
			1             & 基于单个神经元取值   & 神经元覆盖、$k$-多区间覆盖      & $O(n)$          & \cite{ma2018deepgauge}\cite{Pei2019DeepXplore} \\
			2             & 训练集神经元边界     & 神经元边界覆盖、强激活覆盖      & $O(nm)$         & \cite{ma2018deepgauge}                         \\
			3             & 与训练数据分布的距离 & 意外覆盖，平均偏差等            & $O(nm)$         & \cite{Kim2019Guiding}\cite{Tian2019Testing}    \\
			4             & 神经元激活通路覆盖   & 符号-符号覆盖、距离-符号覆盖等  & $O(nl)$         & \cite{Wang2019DeepPath}\cite{Sun2018Testing}   \\
			5             & 神经元的状态转换     & 状态级别覆盖、转换级别覆盖      & $O(n)$          & \cite{Du2018DeepCruiser}                       \\
			6             & 神经元组合测试       & $t$-way组合稀疏覆盖、密集覆盖等 & $O(n^2)$        & \cite{ma2019deepct}                            \\
			\bottomrule
		\end{tabular}
	\end{table}

\fi



%其中涉及到的覆盖指标主要有神经元覆盖率(NC)，$k$区间覆盖率(KMNC)，重要性驱动覆盖
%(IDC)，神经元边界覆盖率(NBC)，强神经元覆盖率(SNC)，重要神经元覆盖(INC)，意外覆
%盖(SC)，神经元激活向量距离(NAVD)，平均偏差(MD)，重要神经元通路覆盖率(INPC)，强
%激活通路覆盖率(SAPC)，符号-符号覆盖(SSC)，距离-符号覆盖(DSC)，符号-值覆盖
%(SVC)，距离-值覆盖(DVC)，状态级别覆盖(BSC)，转换级别覆盖(BTC)，$t-way$组合稀疏
%%%%覆盖($t-way$ CSC)，$t-way$组合密集覆盖($t-way$
%CDC)，$(p,t)$完整性覆盖($(p,t)$ C)等指标。

%DeepXplore~\cite{Pei2019DeepXplore}、DeepGauge~\citess{ma2018deepgauge}、IDC~\citess{Gerasimou2020Importance}
%等工作主要围绕神经元覆盖率、K区间覆盖率、重要性驱动覆盖等测试覆盖指标。


{基于单个神经元取值}，早期针对深度学习模型的测试覆盖指标以神经元为基本单位来评估
测试充分性。Pei等人\citess{Pei2019DeepXplore}首次提出了针对深度学习模型的白盒测试
指标，即神经元覆盖。该方法将取值高于阈值的神经元视为被激活，通过计算模型中被激活神经元
的比例作为测试覆盖率。 Ma等人\citess{ma2018deepgauge}提出了$k$-多区间覆盖指标，
基于每个神经元在训练集上的取值范围，将其划分为多个取值区间，并计算测试集对这些取值区间
的覆盖率。此外，他们还提出了神经元边界覆盖指标和强激活覆盖指标，统计在测试集中神经元的值超过训练
集上下边界的比例。Gerasimou等人\citess{Gerasimou2020Importance}衡量每个
神经元对模型分类结果的影响，提出了基于神经元重要性的测试覆盖标准。

由于单个神经元对模型内部活动状态的描述存在局限性，目前基于多个神经元取值组合的工
作
\citess{Kim2019Guiding,Tian2019Testing,Wang2019DeepPath,Sun2018Testing,ma2019deepct,Xie2021NPC}
较多。Kim等人\citess{Kim2019Guiding}提出了基于“意外度”的测试覆盖指标，衡量模型的测试数据和训练集在推理时隐藏层
向量表示之间的距离。Tian等人\citess{Tian2019Testing}在图像分类任务上提出一种基于神经元激活频率的白盒测试框架，用来检测分
类器中的混淆和偏差错误。Wang等人\citess{Wang2019DeepPath}提出了一组路径驱动的测试度量指标，以识别对抗性样本。Sun等人
\citess{Sun2018Testing}将传统的MC/DC覆盖标准应用于深度神经网络，采用梯度搜索的方法生成测试集。Ma等人
\citess{ma2019deepct}将组合测试应用于神经网络，提出了$t$-way组合稀疏覆盖和密集覆盖等指标。Xie等人\citess{Xie2021NPC}基于
神经网络决策图的控制流和数据流，提出路径覆盖指标来衡量测试集的充分性。

\textbf{尽管现有测试覆盖指标在一定程度上反映了测试数据的多样性，但面临可解释性不足这
	一问题}。无论是基于单个神经元取值还是多个神经元的取值组合，现有测试度量指标研究集
中于对神经元取值的覆盖程度，测试人员无法将测试度量指标和模型内在的功能联系起来，
无法确认测试覆盖率是否反映了测试集对深度学习模型功能的测试充分性。
\textbf{因为缺少对抽象语义表示覆盖的研究，现有覆盖指标对测试充分性的度量缺乏可解释性。}

%现有指标伸缩性较差，其计算时间和指标有效性难以应对实际大规模模型。本项目拟提出
%基于知识萃取的可解释测试覆盖指标，将知识蒸馏和知识回顾应用于神经网络模型测试，
%从而提升测试指标的伸缩性和可解释性。


%，导致在这些指标引导下的测试数据集生成缺乏符合人类认知的测试目的，难以帮助测试人员诊断和调试模型。

\subsubsection{测试数据集生成}

为了提高深度学习模型的可靠性，使用足够的测试数据对其一般行为和各种边界条件下的极端行为进行充分测试是十分必要的。在深度学
习模型测试研究中，如何生成具有代表性的，容易暴露模型潜在错误行为的测试数据已成为深度学习测试的一个研究重点。关于深度学习
模型测试集生成的研究工作可分为\textbf{测试数据生成和优选}两方面。


\begin{table}[htp]
	\renewcommand\arraystretch{1.5}
	\small
	\centering
	\caption{面向深度学习模型的测试数据生成方法}
	\label{tab:testingDataGen}
	% \begin{tabular}{cp{5cm}p{2cm}cp{2cm}}
	\begin{tabular}{cccccc}
		\toprule
		\textbf{序号} & \textbf{算法思想} & \textbf{评价方法}              & \textbf{测试数据} & \textbf{文献号}                                                             \\
		\midrule
		1             & 模糊测试          & 测试覆盖率、效率               & 图像、文本        & \cite{Odena2019TensorFuzz}\cite{Guo2018DLFuzz}\cite{xie2019coverage}        \\
		2             & 符号执行          & 测试覆盖率、像素重要性等       & 图像、代码        & \cite{Gopinath2018Symbolic}\cite{Sun2018Concolic}                           \\
		3             & 对抗攻击          & 准确率、失真度、人类对比评价等 & 图像、文本等      & \cite{Xiao2018Spatially}\cite{Wicker2018FeatureGuided}\cite{He2018Decision} \\
		\bottomrule
	\end{tabular}
\end{table}


在测试数据生成方面，现有方法可分为基于模糊测试、符号执行和对抗攻击这三类方法（见\cref{tab:testingDataGen}）。其中，基于
模糊测试的方法通过随机或者特定规则将输入种子进行变异，观察模型在边界条件下是否会发生错误
\citess{Guo2018DLFuzz,Odena2019TensorFuzz,xie2019coverage,zhang2018deeproad}。Sun等人\citess{Sun2018Concolic}在测试覆盖
指标的基础上，结合具体执行和符号分析生成新的测试数据，以快速提高神经网络的测试覆盖率。Gopinath等人
\citess{Gopinath2018Symbolic}提出了一种轻量级的符号执行技术来测试图像分类模型，解决了重要像素的识别以及创建1像素和2像素
攻击等关键问题。基于对抗样本攻击的方法，很多研究通过向原始样本添加微小扰动的方式生成对抗样本，使深度学习模型做出错误预测
\citess{Xiao2018Spatially,He2018Decision,Wicker2018FeatureGuided}。

\begin{table}[htp]
	\renewcommand\arraystretch{1.5}
	\small
	\centering
	\caption{面向深度学习模型的测试数据优选方法}
	\label{tab:testingDataPri}
	\begin{tabular}{cccc}
		\toprule
		\textbf{序号} & \textbf{算法思想} & \textbf{测试对象} & \textbf{文献号}                                                                                                          \\
		\midrule
		1             & 变异测试方法      & 图像              & \cite{Wang2021Prioritizing}\cite{Ma2018DeepMutation}  \cite{Liu2022DeepState}                                            \\
		2             & 测试数据输出概率  & 图像、自然语言    & \cite{Byun2019Input}\cite{Shen2020MultipleBoundary}\cite{Feng2020DeepGini}\cite{Hu2022AnEmpirical}\cite{Gao2022Adaptive} \\
		\bottomrule
	\end{tabular}
\end{table}

在测试数据优选方面，由于深度学习模型的输入样本空间通常较大，而人工标注测试预言的成本较高，因此很难在系统部署前检测每个输
入样本的正确性。为了解决这个问题，部分研究工作从大规模无标注数据中选择一个测试子集来优先标注和测
试。\cref{tab:testingDataPri}总结了现有的测试数据选择方法，根据算法的思想可分为基于变异测试和模型输出概率的方法。其中，
变异测试通过设计针对深度学习模型的变异算子，从模型结构、参数、权重和输入等方面对模型进行变异，为每个测试数据生成多个模型
变异体，衡量测试数据对模型错误的检测能力，优选能够检测出较多变异体的测试数据来标注和测试
\citess{Ma2018DeepMutation,Wang2021Prioritizing,liu2019exploiting,Liu2022DeepState}。{基于模型的输出概率}，部分研究工作
从置信度、不确定性、意外度、敏感度等角度估算测试数据的错误概率，优选可能导致模型预测错误的测试数据进行标注和测试
\citess{Feng2020DeepGini,Hu2022AnEmpirical}。

\textbf{在基于深度学习模型的安全攸关任务上，测试数据集生成方法DeepSuite\citess{xu2021deepsuite}对自动驾驶软件的转向
	（角）预测模型的测试进行了探索性尝试和研究（申请人作为该文章的第一作者，且文章已收录于IEEE T-ITS，中科院SCI一区期刊）}。该方
法主要基于多目标搜索对测试集种子进行多粒度变异和搜索，生成规模可控的具有高代表性的测试集，并在伯克利
DeepDrive（BDD）数据集和Udacity数据集上证明了该方法所生成的测试集具有高代表性和错误检测能力。\textbf{该研究工作说明申请
	人在该领域有坚实的研究基础}，本项目将会继续沿着该学术方向在以下方面进行更为深入的研究：
\begin{itemize}
	\item \textbf{现有测试度量指标依赖于对网络结构和神经元值的覆盖，缺乏对模型决策行为和决策依据的理解}。与传统软件
	      测试不同，测试人员无法将测试度量指标和模型的待测功能联系起来，因此，虽然在现有度量指标的引导下能够生成导致模型预
	      测错误的测试数据，但测试人员无法根据测试结果反馈进行模型诊断和调试，导致测试方法与实际应用存在一定的距离。
	\item \textbf{现有测试方法存在面对大规模深度学习模型时伸缩性较差的共性问题，复杂的网络结构导致测试可解释性无法得
		      到更为深入的研究}。天津大学刘爽老师团队的综述论文\citess{survey}对深度神经网络测试研究进行了详细的调研和总
	      结，研究表明目前深度神经网络的测试过程需要较大的计算资源，如何提升对大规模深度神经网络的测试和理解，是一项
	      亟待解决的问题。
	\item \textbf{现有测试场景主要集中于白盒测试场景，而对受限场景下深度学习模型的黑盒测试研究较少，更缺乏针对此种情
		      况的可解释性研究}。现有测试方法假设测试人员能够掌握所有的训练数据和整个深度学习模型，但在许多场景
	      下，例如模型为商业公司私有或第三方机构提供时，测试者无法访问训练数据和模型内部结构，如何在黑盒测试
	      场景下对深度学习模型进行可解释测试，具有重要意义。
	\item \textbf{现有的测试集生成方法缺乏具有解释性的测试逻辑，难以提供能够被测试人员和用户所理解的整体检测报告}。面向深
	      度学习模型的系统性测试方法，不仅要评估模型在给定测试集上的性能表现，还需要建立测试结果与待测模型行为之间的
	      内在联系。现有的测试集生成方法缺乏能够被测试人员和用户所理解的测试逻辑，测试人员无法解释测试集的充分性和代
	      表性，用户很难凭借在给定测试集上的测试结果对模型产生足够的信任，针对深度学习模型的自动化可解释性
	      测试亟待研究。
\end{itemize}




% 因为写 demo，我把参考文献放这里了，真写本子的时候，还是要放在国内外概况那边
\begin{spacing}{1.3} % 行距
	\zihao{5} \songti
	\bibliographystyle{gbt7714-nsfc}
	\bibliography{ref,cai_refs}
	\vspace{11bp}
\end{spacing}
