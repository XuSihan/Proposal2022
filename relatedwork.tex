\subsection{国内外相关工作}\label{relatedwork}

% 根据与本项目的相关性，本节从覆盖充分性指标、神经网络测试数据生成、深度学习框架测试以及第三方组件漏洞挖掘四个方面介绍和分析国内外研究现状。

根据与本项目的相关性，本节从可解释性研究、测试度量指标、测试集生成和三个方面介绍
和分析国内外相关工作。

\subsubsection{可解释性研究}
{人工智能的可解释性研究是最近几年人工智能领域和软件工程领域（可信人工智能）的研
究热点}。浙江大学的纪守领老师团队对于目前机器学习和深度学习模型的可解释性研究进
行了详细的调研和总结，调研表明\textbf{目前人工智能的可解释性研究仍旧处于初级阶
段，并且学术界对深度学习模型的可解释性仍旧缺乏统一的认识}\citess{jishouling}。模
型可解释性总体上可分为\textbf{事前可解释性和事后可解释性}。其中，事前可
解释性指通过训练结构简单、可解释性强的模型或将可解释性结合到模型结构中，使模型本
身具备可解释能力。事后可解释是指通过可解释性方法来对已经训练好的模型进行解释
\citess{jishouling}。

事前可解释性无需额外的信息就可以理解模型的决策过程或决策依据，通常采用结构简单、
易于理解的模型来实现，例如线性回归、决策树、朴素贝叶斯等。事前可解释性要求模型的
决策过程或决策依据可模拟，模型的每一个输入以及每一维特征都有直观的解释
~\citess{lou2012intelligible}。对于朴素贝叶斯模型的预测，可以很容易地转化为单个
特征值的贡献~\citess{strumbelj2010efficient}；对于线性模型，其权重则直接反映了特
征重要性~\citess{ribeiro2016should}；对于决策树模型，每一条决策路径所对应的条件
规则都为最终的分类结果提供了解释~\citess{huysmans2011empirical}。为了提高简单线
性模型的准确率，同时保留其事前可解释性，广义加性模型通过简单的线性函数组合每个单
特征模型得到最终决策~\citess{wood2006generalized}。另外，事前可解释性也可通过引
入注意力机制来达到，注意力权重矩阵直接体现了模型在决策过程中感兴趣的区域，从而对
模型进行一定程度的解释~\citess{zhang2021context}。

事后可解释性发生在模型训练之后，通过构建解释方法对模型的工作机制、决策行为和决策
依据进行解释。早期针对事后可解释性的研究主要通过规则提取得到对复杂模型整体决策逻
辑的理解~\citess{bastani2017interpreting}。然而，规则提取只能提供近似解释，不一
定能反映待解释模型的真实行为，且受规则复杂度的制约。当模型的结构过于复杂时，要想
从整体上理解模型的决策逻辑通常是很困难的，解决该问题的一个有效途径是降低待解释模
型的复杂度，而\textbf{知识蒸馏(knowledge distillation)则是降低模型复杂度，提高模型可解释
性的有效方法~\citess{gou2021knowledge}}。{由于知识蒸馏可以完成从教师模型到学生模
型的知识迁移，因而学生模型可以看作是教师模型的全局近似，在一定程度上反映了教师模
型的整体逻辑，因此可以基于学生模型，提供对教师模型的全局解释}。此外，由于整体解
释模型行为较为困难，部分研究以输入样本为导向，通过分析输入样本的每一维特征对模型
最终决策结果的贡献来实现事后可解释性，典型的方法包括敏感性分析解释
~\citess{robnik2008explaining}、局部近似解释~\citess{ribeiro2018anchors}、梯度反
向传播解释~\citess{ding2017visualizing}、特征反演解释
~\citess{dosovitskiy2016inverting}等。然而，现有解释方法很难保证解释结果的准确性
和一致性，为此，Chu等人~\citess{chu2018exact}提出了一种解释方法，可为分段线性神
经网络家族模型提供精确一致的解释，但该方法只能解释线性神经网络模型，无法应用于解
释非线性神经网络模型。

\textbf{尽管已经有很多针对机器学习和深度学习模型本身的可解释性研究，但是对于本项
目关注的针对深度学习模型测试的可解释性研究工作还十分有限。}Kim等人
\citess{Kim2019Guiding}提出一种基于“意外度”的测试覆盖指标，通过测试数据与训练集
在隐藏层表示上的距离，衡量测试数据可能导致模型预测错误的可能性，以此对测试充分性
进行可解释分析。Xie等人\citess{Xie2021NPC}提出了神经元路径覆盖指标，基于神经网络
中决策图的控制流和数据流，提出了两种路径覆盖的变体来衡量测试集的充分性。虽然这些
方法在一定程度上解释了针对深度学习模型的测试覆盖指标，但他们局限于模型推理时神经
元的取值，在高层次语义上对模型决策行为和决策依据的解释不足，测试人员难以根据测试
结果判断模型是否在以符合人类认知的形式正常工作。\textbf{因此，可解释性方法的能力
在深度学习模型测试领域并没有得到准确的说明和研究}。

\iffalse
\textbf{尽管已经有很多针对机器学习和深度学习模型本身的可解释性研究，但是对于本项
目关注的针对深度学习系统测试方面的可解释性研究工作还十分有限。}Kim等人
\citess{Kim2019Guiding}提出一种基于“意外度”的测试覆盖指标，通过度量测试数据与训
练集的不同距离，评估测试集对样本输入空间的覆盖度，从而对测试目标进行一定的可解释
性分析。Xie等人\citess{Xie2021NPC}提出了神经元路径覆盖指标，类似于传统的程序控制
流图，首先从深度神经网络中提取决策图用来表示模型的决策逻辑，然后基于决策图的控制
流和数据流，该方法提出了两种路径覆盖的变体来衡量测试数据在执行决策逻辑时的充分
性。该测试方法在一定程度上反映出模型的决策逻辑，但由于模型本身缺乏可解释性，难以
从控制流或数据流路径上辅助开发人员找到模型失效的原因，从而帮助修复模型。但是该方
法依赖于模型推理过程中的神经元取值，缺乏对模型决策行为的解释，难以检查模型是否在
以符合人类认知的形式正常工作。
\fi

\subsubsection{测试度量指标}
{测试覆盖指标是衡量测试集充分性的标尺，是软件测试的重要研究问题之一。}对于传统软
件，若测试集遍历了待测软件所有的语句、分支和路径，则在一定程度上表明测试集对软件
的功能进行了充分性测试\citess{hilton2018large}。对于深度神经网络而言，其本身的高
维连续特性导致测试集很难遍历所有可能的输入空间，为提高测试集多样性，目前有很多研
究提出了关于深度神经网络的结构覆盖指标，主要分为基于\textbf{单个神经元取值和多个
神经元取值组合}两类。

%，从不同角度测试衡量测试集对模型的覆盖充分性。


\iffalse
\cref{tab:coverage_criteria}总结了现有神经网络测试覆盖指标，其中$m$表示训练
集规模，$n$表示神经元个数，$l$表示神经网络层数。根据覆盖思想的不同，可分为以下六
类：

\begin{table}[htp]
	\renewcommand\arraystretch{1.5}
	\small
	\centering
	\caption{现有神经网络覆盖充分性指标}
	\label{tab:coverage_criteria}
	% \begin{tabular}{p{3cm}p{5cm}p{1cm}p{1cm}p{2cm}}
	\begin{tabular}{ccccc}
		\toprule
		\textbf{序号} & \textbf{主要思想} & \textbf{覆盖指标}       & \textbf{复杂度} & \textbf{文献号}                                                  \\
		\midrule
		1             & 基于单个神经元取值  & 神经元覆盖、$k$-多区间覆盖                    & $O(n)$          & \cite{ma2018deepgauge}\cite{Pei2019DeepXplore} \\
		2             & 训练集神经元边界  & 神经元边界覆盖、强激活覆盖                    & $O(nm)$          & \cite{ma2018deepgauge}                                           \\
		3             & 与训练数据分布的距离  & 意外覆盖，平均偏差等                & $O(nm)$         & \cite{Kim2019Guiding}\cite{Tian2019Testing}            \\
		4             & 神经元激活通路覆盖    & 符号-符号覆盖、距离-符号覆盖等 & $O(nl)$ & \cite{Wang2019DeepPath}\cite{Sun2018Testing} \\
		5             & 神经元的状态转换        & 状态级别覆盖、转换级别覆盖                         & $O(n)$          & \cite{Du2018DeepCruiser}                                         \\
		6             & 神经元组合测试    & $t$-way组合稀疏覆盖、密集覆盖等 & $O(n^2)$ & \cite{ma2019deepct} \\
		\bottomrule
	\end{tabular}
\end{table}

\fi



%其中涉及到的覆盖指标主要有神经元覆盖率(NC)，$k$区间覆盖率(KMNC)，重要性驱动覆盖
%(IDC)，神经元边界覆盖率(NBC)，强神经元覆盖率(SNC)，重要神经元覆盖(INC)，意外覆
%盖(SC)，神经元激活向量距离(NAVD)，平均偏差(MD)，重要神经元通路覆盖率(INPC)，强
%激活通路覆盖率(SAPC)，符号-符号覆盖(SSC)，距离-符号覆盖(DSC)，符号-值覆盖
%(SVC)，距离-值覆盖(DVC)，状态级别覆盖(BSC)，转换级别覆盖(BTC)，$t-way$组合稀疏
%%%%覆盖($t-way$ CSC)，$t-way$组合密集覆盖($t-way$
%CDC)，$(p,t)$完整性覆盖($(p,t)$ C)等指标。

%DeepXplore~\cite{Pei2019DeepXplore}、DeepGauge~\citess{ma2018deepgauge}、IDC~\citess{Gerasimou2020Importance}
%等工作主要围绕神经元覆盖率、K区间覆盖率、重要性驱动覆盖等测试覆盖指标。


{基于单个神经元取值}，早期针对深度学习模型的测试覆盖指标以神经元为基本单位来评估
测试充分性。Pei等人\citess{Pei2019DeepXplore}首次提出了针对深度学习模型白盒测试
指标，即神经元覆盖，将取值高于阈值的神经元视为被激活，通过计算模型中被激活神经元
的比率作为测试覆盖率。 Ma等人\citess{ma2018deepgauge}提出了$k$-多区间覆盖指标，
基于每个神经元在训练集上的取值范围，将其划分为多个取值区间，并计算对神经元值区间
的覆盖率。他们还提出神经元边界覆盖指标和强激活覆盖指标，统计测试集中取值超过训练
集上下边界的神经元的比例。Gerasimou等人\citess{Gerasimou2020Importance}衡量每个
神经元对模型分类结果的影响，提出了基于神经元重要性的测试覆盖标准。

由于单个神经元对模型内部活动状态的描述存在局限性，目前基于多个神经元取值组合的工
作
\citess{Kim2019Guiding,Tian2019Testing,Wang2019DeepPath,Sun2018Testing,ma2019deepct,Xie2021NPC}
占有的比重更大。Kim等人\citess{Kim2019Guiding}提出了“意外”覆盖指标，衡量模型的测
试数据和训练集在推理时隐藏层向量表示之间的距离。Tian等人\citess{Tian2019Testing}
在图像分类任务上提出一种基于神经元激活频率的白盒测试框架，用来检测分类器中的混淆
和偏差错误。Wang等人\citess{Wang2019DeepPath}提出了一组针对神经网络模型的路径驱
动的测试度量指标，以识别对抗性样本。Sun等人\citess{Sun2018Testing}提出了将传统的
MC/DC覆盖标准应用于深度神经网络，采用梯度搜索的方法生成新的测试集。Ma等人
\citess{ma2019deepct}将组合测试应用于神经网络，提出了$t$-way组合稀疏覆盖和密集覆
盖等指标。Xie等人\citess{Xie2021NPC}基于神经网络决策图的控制流和数据流，提出路径
覆盖指标来衡量测试集的充分性。

\textbf{尽管现有测试覆盖指标在一定程度上反映了测试数据的多样性，但面临可解释性这
一问题}。无论是基于单个神经元取值还是多个神经元取值组合，现有测试度量指标研究集
中于对神经元取值的覆盖程度，测试人员无法将测试度量指标和模型内在的功能联系起来，
无法确认测试覆盖率是否反映了测试集对深度学习模型功能的测试充分性。
\textbf{因为缺少对抽象语义表示覆盖的研究，因此现有覆盖指标对测试充分性的度量缺乏可解释性。}

%现有指标伸缩性较差，其计算时间和指标有效性难以应对实际大规模模型。本项目拟提出
%基于知识萃取的可解释测试覆盖指标，将知识蒸馏和知识回顾应用于神经网络模型测试，
%从而提升测试指标的伸缩性和可解释性。


%，导致在这些指标引导下的测试数据集生成缺乏符合人类认知的测试目的，难以帮助测试人员诊断和调试模型。

\subsubsection{测试数据集生成}

为提高深度学习模型的可靠性，使用足够的测试输入对其一般行为和各种边界条件下的行为
进行充分测试是十分必要的。在对深度学习模型的测试中，如何生成更具代表性的和更容易
暴露模型错误行为的测试数据已成为深度学习测试的一个研究重点。关于测试集生成的研究
可分为\textbf{测试数据生成和优选}两方面。


\begin{table}[htp]
	\renewcommand\arraystretch{1.5}
	\small
	\centering
	\caption{测试数据生成方法}
	\label{tab:testingDataGen}
	% \begin{tabular}{cp{5cm}p{2cm}cp{2cm}}
	\begin{tabular}{cccccc}
		\toprule
		\textbf{序号} & \textbf{算法思想} & \textbf{评价方法}               & \textbf{测试数据} & \textbf{文献号}             \\
		\midrule
		1             & 模糊测试          & 测试覆盖率、效率 & 图像、文本 & \cite{Odena2019TensorFuzz}\cite{Guo2018DLFuzz}\cite{xie2019coverage} \\
		2             & 符号执行          & 测试覆盖率、像素重要性等                              & 图像、代码              & \cite{Gopinath2018Symbolic}\cite{Sun2018Concolic} \\
		3             & 对抗样本          & 准确率、失真度、人类对比评价等 & 图像 & \cite{Xiao2018Spatially}\cite{Wicker2018FeatureGuided}\cite{He2018Decision} \\
		\bottomrule
	\end{tabular}
\end{table}


在测试数据生成方面，如\cref{tab:testingDataGen}所示，现有测试数据生成方法可分为
三类。其中，基于模糊测试的方法通过随机或者特定规则将种子输入进行变换，观察模型在
边界条件下是否会发生错误
\citess{Guo2018DLFuzz,Odena2019TensorFuzz,xie2019coverage,zhang2018deeproad}。基
于符号执行的方法，Sun等人\citess{Sun2018Concolic}在测试覆盖指标的基础上，结合具
体执行和符号分析，提高神经网络的测试覆盖率。Gopinath等人
\citess{Gopinath2018Symbolic}提出了一种轻量级的符号执行技术来测试图像分类模型，
解决了重要像素的识别以及创建1像素和2像素攻击等关键问题。基于对抗样本生成的方法，
通过向原始样本添加微小扰动的方式产生对抗样本，使深度学习模型做出错误预测
\citess{Xiao2018Spatially,He2018Decision,Wicker2018FeatureGuided}。

\begin{table}[htp]
	\renewcommand\arraystretch{1.5}
	\small
	\centering
	\caption{测试数据优选方法}
	\label{tab:testingDataPri}
	\begin{tabular}{cccc}
		\toprule
		\textbf{序号} & \textbf{算法思想}  & \textbf{测试对象} & \textbf{文献号}                                                    \\
		\midrule
		1             & 变异测试方法       & 图像              & \cite{Wang2021Prioritizing}\cite{Ma2018DeepMutation}  \cite{Liu2022DeepState}                    \\
		2             & 测试数据输出概率       & 图像、自然语言  & \cite{Byun2019Input}\cite{Shen2020MultipleBoundary}\cite{Feng2020DeepGini}\cite{Hu2022AnEmpirical}\cite{Gao2022Adaptive} \\
		\bottomrule
	\end{tabular}
\end{table}

在测试数据优选方面，由于深度学习模型的输入数据的样本空间通常较大，而人工标注测试
预言的成本较高，因此很难在系统部署前检测每个潜在输入样本的正确性。为解决这个问
题，部分研究工作从大规模无标注数据中选择一个测试子集来优先标注和测
试。\cref{tab:testingDataPri}总结了现有的测试数据选择方法，根据算法的思想可分为
两类。一类是变异测试，通过设计深度学习模型的变异算子，针对每个测试数据生成大量变
异模型，优选能够检测出较多变体的测试数据来标注和测试
\citess{Ma2018DeepMutation,Wang2021Prioritizing,liu2019exploiting,Liu2022DeepState}。
第二类是{基于模型输出概率}的方法，从置信度、不确定性、意外度、敏感度等角度估算测
试数据的错误概率，优选有可能导致模型预测错误的测试数据进行优先标注
\citess{Feng2020DeepGini,Hu2022AnEmpirical}。

\textbf{在基于深度学习模型的安全攸关任务上，测试数据集生成方法
DeepSuite\citess{xu2021deepsuite}对自动驾驶软件的系统性测试方法进行了探索性尝试
和研究（申请人作为该文章的第一作者，且文章已收录于IEEE T-ITS，SCI一区期刊）}。该
方法主要基于多目标搜索对测试集种子进行多粒度变异和搜索，生成规模可控的具有高代表
性的测试集，并在Udacity自动驾驶数据集上证明了该方法所生成的测试集具有高错误检测
能力。\textbf{该研究工作说明申请人在该领域有坚实的研究基础}，本项目将会继续沿着
该学术方向在以下方面进行更为深入的研究：（1）\textbf{现有针对深度学习模型的测试
度量指标依赖于对神经元的值覆盖，缺乏对模型决策行为和决策结果的理解}，虽然在现有
度量指标的引导下能够生成导致模型预测错误的测试数据，但测试人员无法根据测试结果反
馈进行模型诊断和调试。（2）\textbf{现有测试方法存在面对大规模深度学习模型上伸缩
性较差的共性问题，复杂的网络结构导致测试可解释性无法得到更为深入的研究}。天津大
学刘爽老师团队的综述论文\citess{survey}对深度神经网络测试研究进行了详细的调研和
总结，研究表明现有深度神经网络测试过程需要较大的计算资源，如何提升对大规模深度神
经网络的测试和理解，是一项需要被解决的问题。（3）\textbf{现有方法主要关注白盒测
试场景，而对受限场景下深度学习模型的黑盒测试研究较少，更不用说针对此种情况的可解
释性研究}。（4）\textbf{现有测试方法难以提供能够被测试人员和用户理解的整体检测报
告}。面向深度学习模型的测试方法，不仅要评估模型在给定测试集上的性能表现，还需要
建立测试结果与待测模型行为之间的内在联系。现有的测试集生成方法缺乏容易被理解的动
机，测试人员无法理解测试集的充分性和代表性，因此难以根据测试结果预测模型在实际使
用中的效果，针对深度学习模型的自动化可解释性测试亟待研究。



% 因为写 demo，我把参考文献放这里了，真写本子的时候，还是要放在国内外概况那边
\begin{spacing}{1.3} % 行距
	\zihao{5} \songti
	\bibliographystyle{gbt7714-nsfc}
	\bibliography{ref,cai_refs}
	\vspace{11bp}
\end{spacing}
