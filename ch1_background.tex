% !TEX root=./proposal.tex

\subsection{研究意义}


%0.人工智能技术很广泛，智能软件越来越多进入人们的生活1.于此同时，其安全问题也得
%到越来越多的关注2.当前的研究主要针对单个智能组件研究其脆弱性，如对抗攻击，后门
%攻击等。近年来，逐渐有少量研究深度学习基础包和依赖库的漏洞，然而如何利用第三方
%开源组件漏洞去激活智能组件的脆弱性研究较少。
%举例
%挑战：3点
%本项目的研究意义3点

%
% 0.人工智能技术很广泛，智能软件越来越多进入人们的生活
近年来，在数据和算力的驱动下，深度学习取得了巨大的成功。如\cref{fig:ch1:intro}所
示，深度学习通过特征变换和逐层处理，学习输入数据的特征表示，深度学习模型对复杂问
题的解决能力使其研究成果逐渐从实验室走向实际应用，如计算机视觉
\citess{dai2021up}、语音识别\citess{baevski2021unsupervised}和机器翻译
\citess{fan2021beyond}等，并开始部署在自动驾驶\citess{feng2021review}、智慧医疗
\citess{liang2021accurate}和航空航天\citess{julian2019deep}等关键任务上。在获得巨
大成功的同时，深度学习模型的错误行为也导致了很多安全事故。2018年3月19日，在美国
亚利桑那州，一辆处在自动驾驶状态的Uber撞击一名女子，致其不幸身亡，同年7月，Uber
宣布停止研发自动驾驶货车。这些事故发生的根本原因是真实世界可能存在各种各样的模型
输入，难以在所有条件下测试所有可能的测试输入。深度学习系统的错误行为不仅为应用本
身埋下了隐患，也阻碍了深度学习技术在安全攸关任务（如\textbf{交通、医疗、军工}
等）的应用。因此，\textbf{迫切需要在深度学习模型部署前准确评估其性能，找到潜在的
错误行为来预防未知风险}。%习总书记说



%深度学习系统错误的巨大危害催生了学术界和工业界众多检测方法的提出与研究。工业界以DeepMind和特斯拉为代表的众多公司针对深度学习模型

深度学习模型的错误行为带来的巨大危害催生了众多关于深度学习系统检测方法的提出与研
究。 Goodfellow等人\citess{Odena2019TensorFuzz}首次提出了采用模糊测试的方法变异
输入数据，以寻找不符合预期功能的错误行为。加拿大阿尔伯塔大学的Lei Ma团队深入研究
了神经网络测试覆盖指标\citess{ma2018deepgauge,ma2019deepct}和测试数据生成
\citess{xie2019coverage,xie2019deephunter}等问题，将传统软件的边界测试、组合测
试等技术应用到神经网络上。国内南京大学的陈振宇老师团队首次提出了针对循环神经网络
的系统性测试方法\citess{DeepState2022}，并提出了循环神经网络在对话系统、司法文
书等应用上的测试集扩充方法\citess{liu2021dialtest,guo2020taujud}。西安交通大学
的沈超老师团队提出了一种基于搜索的方法测试深度学习框架，设计了针对模型结构、参
数、权重和输入的变异算子，测试出逻辑错误、程序崩溃和数值错误三类缺陷
\citess{guo2020audee}。在测试数据选择方面，北京大学的郝丹老师团队提出了一种针对
深度学习模型的测试数据选择方法，通过测试子集模拟大规模测试集的分布，估算模型性能
\citess{zhou2020cost}。\textbf{面向深度学习系统的测试方法在学术界取得了显著成
效，已成为软件工程领域的新兴研究热点。}

\iffalse

深度学习的目标可定义为训练一个模型${f}$，使得该模型能够适用于真实数据分布
$\mathcal D_{gt}$中任意一个从未见过的数据。为了提高真实部署的可靠性，需要系统测
试深度学习模型$\gamma_{gt}$：$\mathbb{E}_{(x, y) \sim \mathcal{D}_{g t}}
\mathbb{I}[f(x)=y]$。然而，由于客观世界的真实数据分布是未知的，因此通常在测试集
$\mathcal D_{\text{test}}$上评估模型性能$\gamma_{\text {test }}:\left(1
/\left|\mathcal D_{\text {test }}\right|\right) \sum_{(x, y) \in \mathcal
D_{\text {test }}} \mathbb{I}[f(x)=y]$。因此，\textbf{针对深度学习模型的测试目
标}为：
\begin{itemize}
    \item[（1）] 找出使模型做出错误预测的数据$\mathcal D_{\text{failures}}$，即
          $\mathcal D_{\text{failures}}=\{(x, y) | (x, y) \in \mathcal D_{\text{test}}
              \wedge f(x) \neq y\}$;
    \item[（2）] 生成测试数据集$\mathcal{D}_{\text{test}} \sim \mathcal{D}_{\text{gt}}$
          ，以揭示模型在真实数据分布上所期望的性能$\gamma_{gt}$和实际测试集上所表现的
          性能$\gamma_{\text{test}}$之间的差异；
    \item[（3）] 根据测试反馈信息，找到模型在泛化能力上的不足，进一步提升模型性能。
\end{itemize}

\fi

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.9\linewidth]{intro.pdf}
    \caption{面向深度学习系统的测试数据生成与检测}
    \label{fig:ch1:intro}
\end{figure}

如\cref{fig:ch1:intro}所示，面向深度学习系统的测试方法步骤较为明确。在测试覆盖度
量的指引下，生成测试集输入到深度学习模型，比较模型输出与预期是否一致，评估模型性
能，找到与预期不符的模型错误行为。\textbf{虽然，深度学习系统测试的方法能够生成测
试数据并暴露模型错误行为，但是，深度学习系统的测试面临着可解释性不足这一显著问
题。}深度学习技术已经在不同的领域，例如图形图像、自然语言处理、语音识别、自动驾
驶、声纹识别等重要的前言领域取得了显著性效果。但是，{人工智能的可解释性仍旧是一
个十分具有挑战性的问题，得到了众多学者的关注和研究}。在人工智能场景中，可解释性
被定义为“向人类解释或呈现可理解的术语的能力\citess{doshi2017towards}”。深度学习
系统的测试方法同样面临着可解释性不足的问题，\textbf{因为现有测试方法无法对测试结
果给出细致的、具有明确语义的解释，测试人员无法得知测试成功或失效的原因，也不能根
据测试结果定位模型缺陷}，测试人员会产生“为什么这个输入数据会导致模型预测错误？”
的疑问。另一方面，现有测试方法所生成的测试集也缺乏可解释性，\textbf{测试人员无法
将测试集与深度学习系统的待测功能联系起来}，无法确认测试集对深度学习系统的检测能
力，会产生“为什么这个测试集能够衡量模型质量？”的疑问，而用户也很难凭借在任意测试
集上的准确率等单一性能指标而对模型产生足够的信任。因此，\textbf{进行面向深度学习
系统测试的可解释性研究具有十分重要的意义，同时也是人工智能可解释性研究和可信软件
研究的重要组成部分}。









    
    
    



%美国白宫颁布了《维护美国在人工智能领域领导地位》、《国家人工智能研发战略》；欧
%盟致力于打造“从实验室进入市场”，发布《2021人工智能协调计划审查》；俄罗斯发布
%《2030年前国家人工智能发展战略》； 2017年7月，我国国务院印发《新一代人工智能发
%展规划》，旨在构筑我国人工智能发展的先发优势，2019年科技部印发《国家新一代人工
%智能创新发展试验区建设工作指引》，全面提升人工智能创新能力和水平。



%2019年国家新一代人工智能治理专业委员会发布《新一代人工智能治理原则——发展负责任
%的人工智能》，该文件中指出“\textbf{人工智能系统应不断提升透明性、可解释性、可靠
%性、可控性}，逐步实现可审核、可监督、可追溯、可信赖”。

% 2. 现有工作和方法的不足
虽然已有研究在测试度量指标构建、测试输入生成和测试预言生成等方面已取得一定研究成
果，但受传统结构覆盖测试的影响，\textbf{现有研究主要集中于对以神经元为基本单位的
神经网络架构进行结构覆盖测试}，从神经网络和传统软件的主要区别以及深度学习模型测
试的实际需求两方面考虑，现有研究主要存在以下问题：





\textbf{（1）覆盖目标缺乏明确语义，导致测试结果难以辅助开发人员}

随着数据和算力的发展，深度学习模型的规模近年来呈越来越大的趋势，很多研究证明了大
规模模型对复杂问题的解决能力~\cite{kenton2019bert,he2021masked}。然而，现有的深
度学习结构覆盖测试指标大多难以适用于大规模深度学习模型，主要体现在以下两个方面：
第一，随着模型复杂度的提高，神经元及其取值组合的数量也随之提高，以神经元为基础结
构的结构覆盖率和测试多样性之间的相关性越来越小，导致结构覆盖率对测试集质量的评估
能力骤减；第二，现有结构覆盖指标的计算复杂度与神经元的个数或训练集的规模呈线性增
长关系，伸缩性较差，难以适用于实际使用的大规模深度学习模型。


\textbf{（2）在受限场景下，无法拿到神经网络结构，如何}

面向深度学习模型的系统性测试，不仅要评估深度学习模型在特定测试集上的性能表现，还
需要建立测试结果与模型行为之间的联系，从而辅助开发人员修复提升模型。由于深度学习
模型的黑盒性质，现有的基于神经元的结构覆盖指标不能体现深度学习系统的决策逻辑，缺
乏有效语义。虽然在测试覆盖指标的引导下可以生成测试数据，甚至暴露模型的错误行为，
但与传统软件不同的是，开发人员无法知道这些测试数据的成功或失效执行的原因，导致测
试反馈结果很难辅助开发人员定位模型缺陷，提升模型性能。

\textbf{（3）测试集的检测能力缺乏可解释性，难以提供具有可解释性的整体检测报告}

测试用例由输入数据和预期输出两部分组成。在测试一个深度学习模型时，其输入数据的样
本空间经常较大，而人工标注测试预言的成本较高，因此很难在系统部署前检测每个潜在数
据样本的预测正确性。比如图片分类任务中，输入数据是高维连续的图像数据，而目前对图
像的自动标注研究尚在起步阶段，通常需要人工标注图片的类别作为测试用例的预期输出，
在执行测试用例后，根据模型的输出是否符合预期来判断测试用例的成功与否。与此同时，
深度学习模型的测试对测试集的质量有较高要求，即一方面期望测试集能够尽可能模拟真实
数据的分布，准确评估模型性能，另一方面尽早发现容易导致模型预测错误的测试数据，从
而避免在模型部署后仍然存在大量意外输入，导致模型决策错误。为了在有限标注成本内提
高测试的有效性，需要提高测试集的效率，生成规模可控、高质量的测试集。


% 4. 研究方法
深度学习技术正在潜移默化地改变人们的工作和生活方式，其在自动驾驶、恶意软件检测以
及飞机碰撞避免系统等安全攸关领域部署的核心在于“质量”，而深度学习模型的测试技术是
保障深度学习模型质量的必要手段，在这一背景下，\textbf{本项目针对深度学习模型测试
存在的伸缩性差、缺乏可解释性以及标注成本过高的问题，研究适用于大规模神经网络、兼顾测
试成本和测试质量的可解释测试技术，解决了深度神经网络的黑盒复杂特性给模型测试带来的
挑战，为深度学习模型部署在安全攸关领域提供核心技术支撑。}


\input{relatedwork.tex}