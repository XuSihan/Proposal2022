% !TEX root=./proposal.tex

\subsection{研究意义}


%0.人工智能技术很广泛，智能软件越来越多进入人们的生活1.于此同时，其安全问题也得
%到越来越多的关注2.当前的研究主要针对单个智能组件研究其脆弱性，如对抗攻击，后门
%攻击等。近年来，逐渐有少量研究深度学习基础包和依赖库的漏洞，然而如何利用第三方
%开源组件漏洞去激活智能组件的脆弱性研究较少。
%举例
%挑战：3点
%本项目的研究意义3点

%
% 0.人工智能技术很广泛，智能软件越来越多进入人们的生活
近年来，在数据和算力的驱动下，深度学习技术取得了巨大的成功，逐渐从实验室走向实际
应用，如计算机视觉\citess{dai2021up}、语音识别\citess{baevski2021unsupervised}和
机器翻译\citess{fan2021beyond}等，并开始部署在自动驾驶\citess{feng2021review}、
智慧医疗\citess{liang2021accurate}和航空航天\citess{julian2019deep}等关键任务
上。在获得巨大成功的同时，深度学习模型的错误行为也导致了很多安全事故。2018年3月
19日，在美国亚利桑那州，一辆处在自动驾驶状态的Uber撞击一名女子，致其不幸身亡，同
年7月，Uber宣布停止研发自动驾驶货车\textsuperscript{\cite{Uber}}。这些事故发生的
根本原因是真实世界可能存在各种各样的模型输入，难以遍历所有条件对模型进行测试。深
度学习模型的错误行为不仅为应用本身埋下了隐患，也阻碍了深度学习技术在安全攸关任务
（如\textbf{交通、医疗、军工}等）的应用。因此，{迫切需要在深度学习模型部署前准确
评估其性能，找到其潜在的错误行为来预防未知风险}。\textbf{习总书记指出，要加强人工
智能发展的潜在风险研判和防范，维护人民利益和国家安全，确保人工智能安全、可靠、可
控}。




%深度学习系统错误的巨大危害催生了学术界和工业界众多检测方法的提出与研究。工业界以DeepMind和特斯拉为代表的众多公司针对深度学习模型

深度学习模型错误行为带来的巨大危害催生了众多关于深度学习模型的测试方法的研究。
\textbf{在国外}，Goodfellow等人\citess{Odena2019TensorFuzz}首次提出采用模糊测
试的方法变异输入数据，以寻找不符合预期功能的错误行为。加拿大阿尔伯塔大学的Lei Ma
团队深入研究了深度学习模型的测试覆盖指标\citess{ma2018deepgauge,ma2019deepct}和
测试数据生成\citess{xie2019coverage,xie2019deephunter}等问题，将传统软件的边界测
试、组合测试等技术应用到深度学习模型上。\textbf{在国内}，南京大学的陈振宇老师团
队首次提出了针对循环神经网络的系统性测试方法\citess{DeepState2022}，并在对话系
统、司法文书等应用上提出了新的测试集扩充方法
\citess{liu2021dialtest,guo2020taujud}。西安交通大学的沈超老师团队和天津大学的李
晓红老师团队提出了基于搜索的方法来测试深度学习框架，设计了针对模型结构、参数、权
重和输入的变异算子，能够准确检测出包括逻辑错误、程序崩溃和数值错误在内的三种缺陷
\citess{guo2020audee}。在测试数据选择方面，北京大学的郝丹老师团队提出了一种针对
深度学习模型的测试数据选择方法，通过测试子集的输出概率分布模拟大规模测试集的分
布，估算模型性能\citess{zhou2020cost}。\textbf{面向深度学习模型的测试方法在学术
界取得了显著成效，已成为软件工程领域的新兴研究热点。}

\iffalse
    深度学习的目标可定义为训练一个模型${f}$，使得该模型能够适用于真实数据分布
    $\mathcal D_{gt}$中任意一个从未见过的数据。为了提高真实部署的可靠性，需要系统测
    试深度学习模型$\gamma_{gt}$：$\mathbb{E}_{(x, y) \sim \mathcal{D}_{g t}}
        \mathbb{I}[f(x)=y]$。然而，由于客观世界的真实数据分布是未知的，因此通常在测试集
    $\mathcal D_{\text{test}}$上评估模型性能$\gamma_{\text {test }}:\left(1
        /\left|\mathcal D_{\text {test }}\right|\right) \sum_{(x, y) \in \mathcal
            D_{\text {test }}} \mathbb{I}[f(x)=y]$。因此，\textbf{针对深度学习模型的测试目
        标}为：
    \begin{itemize}
        \item[（1）] 找出使模型做出错误预测的数据$\mathcal D_{\text{failures}}$，即
              $\mathcal D_{\text{failures}}=\{(x, y) | (x, y) \in \mathcal D_{\text{test}}
                  \wedge f(x) \neq y\}$;
        \item[（2）] 生成测试数据集$\mathcal{D}_{\text{test}} \sim \mathcal{D}_{\text{gt}}$
              ，以揭示模型在真实数据分布上所期望的性能$\gamma_{gt}$和实际测试集上所表现的
              性能$\gamma_{\text{test}}$之间的差异；
        \item[（3）] 根据测试反馈信息，找到模型在泛化能力上的不足，进一步提升模型性能。
    \end{itemize}

\fi

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.95\linewidth]{intro.pdf}
    \caption{深度学习模型的测试过程示意}
    \label{fig:ch1:intro}
\end{figure}
%{其可解释性仍旧是一个十分具有挑战性的问题，得到了众多学者的关注和研究}。

\cref{fig:ch1:intro}展示了深度学习模型的一般测试流程。首先，根据测试目标不同，测
试人员会选取不同的测试指标，并基于该测试指标选择或生成对应的测试数据，形成测试
集，输入到深度学习模型，验证模型性能或找到与预期不符的错误行为。\textbf{虽然，现
有测试方法能够在一定程度上反映模型质量或者暴露模型的错误行为，但是，深度学习模型
的测试面临着可解释性不足这一显著问题。}一般来说，可解释性被定义为“向人类解释或呈
现可理解的术语的能力\citess{doshi2017towards}”。在深度学习模型测试的场景中，可解
释性是指向测试人员呈现可以理解的测试反馈的能力。

现有针对深度学习模型的测试方法面临着可解释性不足的问题。具体而言，\textbf{现有方
法无法对测试结果给出细致的、具有明确语义的解释，测试人员无法得知测试成功或失效的
原因，难以根据测试结果来调试和诊断模型}，会产生“为什么这个输入数据会导致模型预测
错误？”的疑问。此外，现有测试集生成方法也不具备解释性，\textbf{测试人员无法将测
试集的验证能力与深度学习模型的待测功能联系起来}，会产生“为什么这个测试集足够衡量
模型质量？”的疑问，而用户也很难在不理解测试数据选择方法的前提下仅凭借准确率等单
一性能指标而对模型产生足够的信任。因此，\textbf{面向深度学习模型的可解释测试研究
具有十分重要的意义，同时也是人工智能可解释性研究和可信软件研究的重要组成部分}。















%美国白宫颁布了《维护美国在人工智能领域领导地位》、《国家人工智能研发战略》；欧
%盟致力于打造“从实验室进入市场”，发布《2021人工智能协调计划审查》；俄罗斯发布
%《2030年前国家人工智能发展战略》； 2017年7月，我国国务院印发《新一代人工智能发
%展规划》，旨在构筑我国人工智能发展的先发优势，2019年科技部印发《国家新一代人工
%智能创新发展试验区建设工作指引》，全面提升人工智能创新能力和水平。



%2019年国家新一代人工智能治理专业委员会发布《新一代人工智能治理原则——发展负责任
%的人工智能》，该文件中指出“\textbf{人工智能系统应不断提升透明性、可解释性、可靠
%性、可控性}，逐步实现可审核、可监督、可追溯、可信赖”。

% 2. 现有工作和方法的不足



\input{relatedwork.tex}