与现有研究工作相比，本项目的特色与创新之处体现在以下几方面：

\begin{itemize}
    \item[(1)] \textbf{本项目提出的针对深度学习模型的可解释测试研究本身具有创新
          性}。赋予人工智能算法可解释研究以具体的应用场景，在多种测试应用场景
          下，对可解释性进行深入研究。{深度学习模型的可解释测试研究，对设计更具
          有测试逻辑的测试目标，解释测试集的充分性和代表性，理解测试执行失效的原
          因，以及辅助诊断和调试模型都具有重要意义。推动用户更好地信任深度学习模
          型的测试结果。}该项研究将会成为人工智能可解释性和可信软件工程的重要组
          成部分，对可解释性研究产生非常积极的作用。
    \item[(2)] 针对现有测试覆盖指标缺乏可解释性、伸缩性差的共性问题，\textbf{提
              出基于层次语义理解的测试覆盖目标抽取方法，构建基于层次语义逻辑的覆
              盖性测试方法，可以填补可解释测试领域国内外研究空白}。现有白盒测试
              方法主要依赖于对神经元的值和取值组合的覆盖，由于深度学习模型包含较
              多神经元，\textbf{现有测试覆盖指标计算复杂度高，不具备可解释性，缺
              乏实际应用价值}。因此，本项目利用知识回顾的方法，分层抽取深度学习
              模型中的决策语义，并将抽取得到的多层语义信息融合到知识蒸馏中，进一
              步提高白盒测试的伸缩性，以适用于较大的深度学习模型。该研究可显著提
              高深度学习白盒测试的可解释性和伸缩性，是本项目相比于现有白盒测试方
              法的重要创新之处。
    \item[(2)] \textbf{针对受限场景下测试者无法访问模型内部结构的问题，本项目研
              究基于知识蒸馏的决策行为模拟方法，提高黑盒测试的有效性和可解释
              性}。目前针对深度学习模型黑盒测试的研究比较缺乏，仅有的研究是对与
              模型无关的测试集多样性分析，缺乏对模型测试行为的建模和理解，一方面
              难以提供对模型决策逻辑和决策依据的测试覆盖信息，测试人员无法准确判
              断测试充分性和代表性；另一方面，现有方法无法为模型开发提供有效的测
              试反馈信息。因此，在黑盒测试场景中，本项目利用知识蒸馏，从模型预测
              的概率分布出发，学习得到一个可解释的学生模型，通过全局近似模拟黑盒
              模型的决策行为，构建伸缩性强、可解释的黑盒覆盖测试指标，是本项目一
              大特色和创新之处。

    \item[(4)] 针对测试数据选取和测试结果反馈缺乏可解释性的问题，\textbf{本项目
              在可解释测试覆盖指标的基础上，研究融合基于实例的解释方法和反馈偏
              置，自适应地持续选取测试数据}。现有测试集生成方法的研究缺乏可解释
              性，导致测试人员针对深度学习模型的测试执行和测试数据选取缺乏理解，
              难以有效辅助诊断和调试模型。本项目在可解释黑盒测试和白盒测试的基础
              上，得到测试数据的决策路径分布，融合基于实例的解释方法和反馈偏置选
              取代表数据和边界数据，生成具有可解释性的测试集。\textbf{项目研究成
              果可为测试人员提供单个测试数据执行失效的原因，同时也提供整个测试集
              选取的解释方法，相比较于现有测试方法更具有可解释性和实用性，可提高
              深度学习模型测试的可信度}。
\end{itemize}