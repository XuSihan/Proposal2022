与现有研究工作相比，本项目的特色与创新之处体现在以下几方面：

\begin{itemize}
    \item[(1)] \textbf{针对黑盒测试中测试者无法访问模型内部结构的问题，本项目研
    究基于知识蒸馏的决策行为模拟方法，提高黑盒测试的有效性和可解释性}。目前针对
    深度学习黑盒测试的研究局限于模型无关的测试用例集多样性分析，缺乏对模型预测行
    为的建模，无法为模型开发提供有效的测试反馈信息；另一方面，仅分析测试用例集和
    模型的输入输出也难以针对模型构建可解释的测试覆盖指标。因此，在黑盒测试场景
    中，本项目利用知识蒸馏，从模型预测的概率分布出发，学习得到一个可解释``学生''
    模型，以模拟黑盒模型的决策行为，进而构建伸缩性强、可解释的覆盖测试指标，是本
    项目一大特色和创新之处。
    \item[(2)] \textbf{针对白盒测试中深度学习模型的可解释性差的问题，本项目研究
    基于层次语义理解的决策路径抽取方法，构建基于决策路径的覆盖性测试方法，提高白
    盒测试的伸缩性和可解释性}。现有白盒测试方法主要基于神经元取值和神经元覆盖度
    进行分析计算，由于深度学习模型包含较多神经元，现有方法计算复杂度高、缺乏可解
    释性，不具有实际应用价值。因此，在白盒测试场景中，本项目利用知识回顾的方法，
    分层抽取深度学习模型中的决策语义，并将抽取得到的多层语义信息融合到知识蒸馏
    中，进一步提高白盒测试的伸缩性，以适用于较大的深度学习模型。该研究可显著提高
    深度学习白盒测试的可解释性，是本项目相比于现有白盒测试方法的重要创新之处。
    \item[(3)] \textbf{针对测试数据生成和测试结果反馈缺乏可解释性的问题，本项目
    在路径覆盖指标的基础上，研究融合反馈偏置的自适应测试数据持续选取策略}。现有
    测试数据生成研究缺乏可解释性，导致针对深度学习的有效测试信息较少，难以有效辅
    助开发人员修复模型缺陷。本项目在可解释黑盒测试和白盒测试的基础上，可以得到测
    试数据的决策路径分布，进而可根据决策路径分布和已测试样本的反馈偏置选取最具代
    表的测试数据，为测试人员提供全阶段的可解释测试方法，是本项目另一个重要的特色
    与创新之处。
\end{itemize}